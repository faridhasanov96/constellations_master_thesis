{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Db1D3tJT18O9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_model import *\n",
    "from supporting_models import * #discriminator from GAN and classifier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "classifier = Network()\n",
    "classifier.to(device)\n",
    "discriminator = Discriminator(1).to(device)\n",
    "\n",
    "vae_generator = ConvVAE()\n",
    "vae_generator.to(device)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HFuoKq18PB"
   },
   "source": [
    "Next, we load the pretrained weights for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpBa66qW18PB",
    "outputId": "e45bbab2-77c6-4ff1-cb03-8149ab44bf89"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "\n",
    "g_state_dict = torch.load(\n",
    "    str(\"conv_vae300.pth\"),\n",
    "    map_location=device,\n",
    ")\n",
    "d_state_dict = torch.load(\n",
    "    'best_gan_weights/mnist_dis_epoch_91.pth',\n",
    "    map_location=device,\n",
    ")\n",
    "cs_state_dict = torch.load(\n",
    "    \"lsi_mnist_weights/classifier32.pth\",\n",
    "    map_location=device,\n",
    ")\n",
    "classifier.load_state_dict(cs_state_dict)\n",
    "discriminator.load_state_dict(d_state_dict)\n",
    "\n",
    "vae_generator.load_state_dict(g_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ7TcVzs1EV8"
   },
   "source": [
    "## Adding specific loss related to how many dots does contour of image pass through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo0MEN__1D-9"
   },
   "outputs": [],
   "source": [
    "from constellation_noise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location = 'augmented_6.jpg'\n",
    "noise_dotted = constellation_create('mnist/mnist/training/6/106.jpg',13,1,0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "mass = mpimg.imread(location)\n",
    "\n",
    "def get_position(np_array):\n",
    "    return ndi.center_of_mass(np_array)\n",
    "\n",
    "get_position(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m62VxYzo6UqI",
    "outputId": "6a740419-a069-4c86-a09b-42b3b0c6f80c"
   },
   "outputs": [],
   "source": [
    "def pass_dots_loss(image_noise_dots, image):\n",
    "  noise_dots = stimuli_dots(image_noise_dots)\n",
    "  gray = drawing_figure(image)\n",
    "\n",
    "  d_img = points_on_image(gray,noise_dots,10)\n",
    "  loss_1 = len(d_img)/len(noise_dots)\n",
    "  return loss_1\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import atan2, cos, sin, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def get_angle(numpy_array):\n",
    "    _, bw = cv2.threshold(numpy_array, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    needed_angle = 0\n",
    "    biggest_area = 0\n",
    "    for i, c in enumerate(contours):\n",
    "\n",
    "      area = cv2.contourArea(c)\n",
    "      \n",
    "      if area > biggest_area: #or 100000 < area:\n",
    "        biggest_area = area\n",
    "        rect = cv2.minAreaRect(c)\n",
    "        center = (int(rect[0][0]),int(rect[0][1])) \n",
    "        width = int(rect[1][0])\n",
    "        height = int(rect[1][1])\n",
    "        angle = int(rect[2])\n",
    "        if width < height:\n",
    "            angle = 90 - angle\n",
    "        else:\n",
    "            angle = -angle\n",
    "        needed_angle = angle\n",
    "    return needed_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(imgs, classifier,shape):\n",
    "    return torch.max(classifier(imgs), 1).indices.cpu().numpy().reshape(shape,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    return img_tensors * 2 -1 \n",
    "\n",
    "#prediction_pil_img(denorm(generated_imgs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_realness(generated_imgs,discriminator):\n",
    "    return discriminator(generated_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_image_generator(immage,distance,noise,times):\n",
    "\n",
    "    archive = GridArchive(\n",
    "    dims = [150,70,6],  # 200 bins in each dimension.\n",
    "    ranges = [(0,500),(0.1,0.7),(0,9)],)\n",
    "\n",
    "    emitters = [\n",
    "    ImprovementEmitter(\n",
    "        archive,\n",
    "        np.zeros(16),\n",
    "        0.1,\n",
    "        batch_size=30,) for _ in range(5)]\n",
    "\n",
    "    optimizer = Optimizer(archive, emitters)\n",
    "    noise_dotted = constellation_create(immage,distance,1,noise)\n",
    "\n",
    "    total_itrs = times\n",
    "    flat_img_size = 32*32  # 28 * 28\n",
    "    start_time = time.time()\n",
    "    class_scores = []\n",
    "    values = []\n",
    "    images_through_training = []\n",
    "    for itr in tqdm(range(1, total_itrs + 1)):\n",
    "\n",
    "        sols = optimizer.ask()\n",
    "        with torch.no_grad():\n",
    "            tensor_sols = torch.tensor(\n",
    "                sols,\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "            generated_imgs = vae_generator.decoder(tensor_sols)\n",
    "            classes = get_prediction(denorm(generated_imgs), classifier,150)\n",
    "            digit_realness_scores = digit_realness(generated_imgs,classifier)\n",
    "            normalized_imgs = (generated_imgs + 1.0) / 2.0\n",
    "\n",
    "            dot_loss_scores = []\n",
    "            for img in generated_imgs:\n",
    "              t = img[0].cpu().numpy()*255\n",
    "              t = t.astype('uint8')\n",
    "              score=pass_dots_loss(noise_dotted,t)\n",
    "              dot_loss_scores.append(score)\n",
    "            \n",
    "            dot_loss_scores = np.array(dot_loss_scores,dtype = np.float32)\n",
    "            \n",
    "            flattened_imgs = normalized_imgs.cpu().numpy().reshape((-1, flat_img_size))\n",
    "            boldness = np.count_nonzero(flattened_imgs >= 0.5,axis=1,keepdims=True)\n",
    "            objs = 0.9*digit_realness_scores+0.1*dot_loss_scores\n",
    "            dot_loss_scores = dot_loss_scores.reshape(150,1)\n",
    "\n",
    "            bcs = np.concatenate([boldness,dot_loss_scores,classes], axis=1)\n",
    "        optimizer.tell(objs, bcs)\n",
    "        \n",
    "    new_archive = archive.as_pandas().sort_values(by=['objective'], ascending = False)\n",
    "    array = np.array(new_archive.iloc[0, 7:])\n",
    "    tensor_sols = torch.tensor(\n",
    "            array,\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        )\n",
    "    output = vae_generator.decoder(tensor_sols)\n",
    "    return noise_dotted,(output[0],new_archive.iloc[0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "imagess = glob.glob('mnist_together/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_images = []\n",
    "top_image = []\n",
    "for img in imagess:\n",
    "    noised, tuple_image_score = inference_image_generator(img,13,0.003,350)\n",
    "    top_image.append(tuple_image_score)\n",
    "    noised_images.append(noised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "imgs_drawn = []\n",
    "#transform = transforms.Resize((64,64))\n",
    "def drawing_figure(image):\n",
    "    gray = image \n",
    "    gray = cv2.resize(gray,(800,800))\n",
    "    th, threshed = cv2.threshold(gray, 40, 255,cv2.THRESH_BINARY)\n",
    "    cnts = cv2.findContours(threshed, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)[-2]\n",
    "    sc =[]\n",
    "    for ind,c in enumerate(cnts):\n",
    "        area = cv2.contourArea(c)\n",
    "        #print(area)\n",
    "        if(area<25000): continue\n",
    "        sc.append(c.copy())\n",
    "    draw_img = np.zeros(gray.shape)\n",
    "    for s in sc:\n",
    "        for pt in s:\n",
    "            draw_img[pt[0][1],pt[0][0]] = 255\n",
    "    return draw_img, sc\n",
    "for i in range(len(top_image)):\n",
    "    noise_dotted_for_drawing = cv2.resize(noised_images[i], dsize=(800,800))\n",
    "    t = top_image[i][0][0].detach().cpu().numpy()*255\n",
    "    t = t.astype('uint8')\n",
    "    contoured_img, hh = drawing_figure(t) \n",
    "    #plt.imshow(contoured_img)\n",
    "    im2 = cv2.drawContours(noise_dotted_for_drawing, hh, -1, (255, 255, 255), 8)\n",
    "    im2 = torch.tensor(im2).unsqueeze(0)\n",
    "    #im2 = transform(im2)\n",
    "    imgs_drawn.append(im2)#im2 = cv2.resize(im2, dsize=(120,120))\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "img_grid1 = make_grid(imgs_drawn, nrow=10, padding=10,pad_value=255)\n",
    "ax = plt.imshow(np.transpose(img_grid1.detach().cpu().numpy(), (1, 2, 0)),cmap='gray')\n",
    "plt.axis('off')\n",
    "#fig.savefig('drawn_vae.png', bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lsi_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
