{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import functools\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from skimage import io, transform\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b72b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05547830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformations import *\n",
    "DIR = 'datasets/dotted_noise_sketches_13_003/combined/'\n",
    "n_gpus = 1\n",
    "batch_size = 4 * n_gpus\n",
    "\n",
    "train_ds = ImageFolder(DIR, transform=transforms.Compose([\n",
    "        Train_Normalize()]))\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b86cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'datasets/dotted_noise_sketches_13_003/combined1/'\n",
    "\n",
    "batch_size = 20 * n_gpus\n",
    "\n",
    "val_ds = ImageFolder(DIR, transform=transforms.Compose([\n",
    "        Val_Normalize()]))\n",
    "val_dl = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inputs, target, figsize=(10, 5)):\n",
    "    inputs = np.uint8(inputs)\n",
    "    target = np.uint8(target)\n",
    "    tar = np.rollaxis(target[0], 0, 3)\n",
    "    inp = np.rollaxis(inputs[0], 0, 3)\n",
    "    title = ['Input Image', 'Ground Truth']\n",
    "    display_list = [inp, tar]\n",
    "    plt.figure(figsize=figsize)\n",
    "  \n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(display_list[i])\n",
    "    plt.axis('off')\n",
    " \n",
    "    #plt.imshow(image)    \n",
    "\n",
    "def show_batch(dl):\n",
    "    j=0\n",
    "    for (images_a, images_b), _ in dl:\n",
    "        j += 1\n",
    "        imshow(images_a, images_b)\n",
    "        if j == 3:\n",
    "            break\n",
    "#show_batch(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f830d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(net, init_type='normal', scaling=0.02):\n",
    "\n",
    "    def init_func(m): \n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv')) != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 0.0, scaling)\n",
    "        elif classname.find('BatchNorm2d') != -1\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, scaling)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  \n",
    "def get_norm_layer():\n",
    "    \n",
    "    norm_type = 'batch'\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bf945",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.device_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ce605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import *\n",
    "norm_layer = get_norm_layer()\n",
    "generator = UnetGenerator(3,3, 64, norm_layer=norm_layer, use_dropout=False)#.cuda().float()\n",
    "generator.apply(weights_init)\n",
    "generator = torch.nn.DataParallel(generator)  # multi-GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if continue training\n",
    "gen_state_dict = torch.load(\n",
    "    'weights_pix2pix/oweights/generator_epoch_25.pth',\n",
    "    map_location=device,\n",
    ")\n",
    "generator.load_state_dict(gen_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c851592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discriminator import *\n",
    "discriminator = Discriminator(6, 64, n_layers=3, norm_layer=norm_layer)\n",
    "discriminator.apply(weights_init)\n",
    "discriminator = torch.nn.DataParallel(discriminator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if continue training\n",
    "dic_state_dict = torch.load(\n",
    "    'weights_pix2pix/oweights/discriminator_epoch_25.pth',\n",
    "    map_location=device,\n",
    ")\n",
    "discriminator.load_state_dict(dic_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss() \n",
    "l1_loss = nn.L1Loss()\n",
    "def generator_loss(generated_image, target_img, G, real_target):\n",
    "    gen_loss = adversarial_loss(G, real_target)\n",
    "    l1_l = l1_loss(generated_image, target_img)\n",
    "    gen_total_loss = gen_loss + (100 * l1_l)\n",
    "    #print(gen_loss)\n",
    "    return gen_total_loss\n",
    "def discriminator_loss(output, label):\n",
    "    disc_loss = adversarial_loss(output, label)\n",
    "    return disc_loss\n",
    "learning_rate = 2e-4\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "g_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(G_optimizer,  mode='min', factor=0.5,patience=5,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 100\n",
    "D_loss_plot, G_loss_plot = [], []\n",
    "for epoch in range(1, num_epochs+1): \n",
    "  \n",
    "    start = time.time()\n",
    "\n",
    "    D_loss_list, G_loss_list = [], []\n",
    "   \n",
    "    for (input_img, target_img), _ in train_dl:\n",
    "       \n",
    "        D_optimizer.zero_grad()\n",
    "        input_img = input_img.to(device)\n",
    "        target_img = target_img.to(device)       \n",
    "        generated_image = generator(input_img)\n",
    "        \n",
    "        disc_inp_fake = torch.cat((input_img, generated_image), 1)\n",
    "       \n",
    "        \n",
    "        \n",
    "        real_target = Variable(torch.ones(input_img.size(0), 1, 30, 30).to(device))\n",
    "        fake_target = Variable(torch.zeros(input_img.size(0), 1, 30, 30).to(device))        \n",
    "        D_fake = discriminator(disc_inp_fake.detach())\n",
    "        \n",
    "        D_fake_loss   =  discriminator_loss(D_fake, fake_target)\n",
    "        disc_inp_real = torch.cat((input_img, target_img), 1)\n",
    "        \n",
    "                                         \n",
    "        output = discriminator(disc_inp_real)\n",
    "        D_real_loss = discriminator_loss(output,  real_target)\n",
    "   \n",
    "        D_total_loss = (D_real_loss + D_fake_loss) / 2\n",
    "        D_loss_list.append(D_total_loss)\n",
    "      \n",
    "        D_total_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "        fake_gen = torch.cat((input_img, generated_image), 1)\n",
    "        G = discriminator(fake_gen)\n",
    "        G_loss = generator_loss(generated_image, target_img, G, real_target)                                 \n",
    "        G_loss_list.append(G_loss)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "    \n",
    "    g_scheduler.step(G_loss)\n",
    "    end = time.time()\n",
    "    print('time spent for epoch:{}'.format(end-start))\n",
    "    print('Epoch: [%d/%d]: D_loss: %.3f, G_loss: %.3f' % (\n",
    "            (epoch), num_epochs, torch.mean(torch.FloatTensor(D_loss_list)),\\\n",
    "             torch.mean(torch.FloatTensor(G_loss_list))))\n",
    "    \n",
    "    D_loss_plot.append(torch.mean(torch.FloatTensor(D_loss_list)))\n",
    "    G_loss_plot.append(torch.mean(torch.FloatTensor(G_loss_list)))\n",
    "    \n",
    "    torch.save(generator.state_dict(), 'weights_pix2pix/oweights/generator_epoch_%d.pth' % (epoch))\n",
    "    torch.save(discriminator.state_dict(), 'weights_pix2pix/oweights/discriminator_epoch_%d.pth' % (epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(D_loss_plot, color='orange', label='d_loss')\n",
    "plt.plot(G_loss_plot, color='red', label='gloss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "    #plt.savefig('outputs_vae/loss.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c87503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing_figure(image):\n",
    "    gray = image \n",
    "    gray = cv2.resize(gray,(800,800))\n",
    "    th, threshed = cv2.threshold(gray, 40, 255,cv2.THRESH_BINARY)\n",
    "    cnts = cv2.findContours(threshed, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)[-2]\n",
    "    sc =[]\n",
    "    for ind,c in enumerate(cnts):\n",
    "        area = cv2.contourArea(c)\n",
    "        #print(area)\n",
    "        if(area<25000): continue\n",
    "        sc.append(c.copy())\n",
    "    draw_img = np.zeros(gray.shape)\n",
    "    for s in sc:\n",
    "        for pt in s:\n",
    "            draw_img[pt[0][1],pt[0][0]] = 255\n",
    "    return draw_img, sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad499dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pair = next(iter(val_dl))\n",
    "\n",
    "predictions = generator(img_pair[0][0])\n",
    "preds = []\n",
    "for img in predictions:\n",
    "    img = (img+1)/2\n",
    "    preds.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92804e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "figsize=(20, 10)\n",
    "plt.figure(figsize=figsize)\n",
    "img_grid = make_grid(predictions, nrow=10, padding=5,pad_value=1)\n",
    "plt.imshow(np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0)),interpolation='nearest',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = []\n",
    "for img in img_pair[0][1]:\n",
    "    img = (img +1)/2\n",
    "    real_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "figsize=(20, 10)\n",
    "plt.figure(figsize=figsize)\n",
    "img_grid = make_grid(real_images, nrow=10, padding=10,pad_value=1)\n",
    "plt.imshow(np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0)),interpolation='nearest',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_noised_images = []\n",
    "for img in img_pair[0][0]:\n",
    "    img = (img +1)/2\n",
    "    real_noised_images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f20cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "figsize=(20, 10)\n",
    "plt.figure(figsize=figsize)\n",
    "img_grid = make_grid(real_noised_images[:10], nrow=10, padding=10,pad_value=1)\n",
    "plt.imshow(np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0)),interpolation='nearest',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_drawn = []\n",
    "for i in range(len(real_images[:10])):\n",
    "    noise_dotted_for_drawing = cv2.resize(real_noised_images[i][0].numpy(), dsize=(800,800))\n",
    "    t = real_images[i][0].detach().cpu().numpy()*255\n",
    "    t = t.astype('uint8')\n",
    "    contoured_img, hh = drawing_figure(t) \n",
    "    im2 = cv2.drawContours(noise_dotted_for_drawing, hh, -1, (255, 255, 0), 8)\n",
    "    im2 = torch.tensor(im2).unsqueeze(0)\n",
    "    imgs_drawn.append(im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bb618",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20, 10)\n",
    "plt.figure(figsize=figsize)\n",
    "img_grid1 = make_grid(imgs_drawn, nrow=10, padding=0)\n",
    "plt.imshow(np.transpose(img_grid1.detach().cpu().numpy(), (1, 2, 0)),cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_drawn = []\n",
    "\n",
    "for i in range(len(predictions[:10])):\n",
    "    noise_dotted_for_drawing = cv2.resize(real_noised_images[i][0].numpy(), dsize=(800,800))\n",
    "    t = predictions[i][0].detach().cpu().numpy()*255\n",
    "    t = t.astype('uint8')\n",
    "    contoured_img, hh = drawing_figure(t) \n",
    "    im2 = cv2.drawContours(noise_dotted_for_drawing, hh, -1, (255, 255, 0), 8)\n",
    "    im2 = torch.tensor(im2).unsqueeze(0)\n",
    "    imgs_drawn.append(im2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20, 10)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "img_grid1 = make_grid(imgs_drawn, nrow=10, padding=0)\n",
    "ax = plt.imshow(np.transpose(img_grid1.detach().cpu().numpy(), (1, 2, 0)),cmap='gray')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
