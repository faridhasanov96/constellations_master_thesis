{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "import glob\n",
    "import random\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, unaligned=False, mode='train'):\n",
    "        self.transform = torchvision.transforms.Compose(transform)\n",
    "        self.unaligned = unaligned\n",
    "        self.train = (mode == 'train')\n",
    "        fl_A = np.random.choice(glob.glob(os.path.join(root_dir, '%sA' % mode) + '/*.*'),8000)\n",
    "        fl_B = np.random.choice(glob.glob(os.path.join(root_dir, '%sB' % mode) + '/*.*'),12000)\n",
    "\n",
    "        self.files_A = sorted(fl_A)\n",
    "        self.files_B = sorted(fl_B)\n",
    "        #print(self.files_A)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "\n",
    "        if self.unaligned:\n",
    "            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n",
    "        else:\n",
    "            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
    "\n",
    "        if self.train:\n",
    "            return {'trainA': item_A, 'trainB': item_B}\n",
    "        else:\n",
    "            return {'testA': item_A, 'testB': item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from models import *\n",
    "\n",
    "FLAGS = None\n",
    "device = torch.device(\"cuda\" if True else \"cpu\")\n",
    "img_size = 128\n",
    "data_dir = 'dataset_cyclegan/'\n",
    "dataset = 'mnist_noise_8_003'\n",
    "channels = 1\n",
    "num_blocks = 9\n",
    "lr = 0.0002\n",
    "print('Loading data...\\n')\n",
    "\n",
    "transform = [transforms.Resize(int(img_size*1.12), Image.BICUBIC),\n",
    "                     transforms.RandomCrop((img_size, img_size)),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.5), (0.5))]\n",
    "dataloader = DataLoader(ImageDataset(os.path.join(data_dir,dataset),\n",
    "                                             transform=transform, unaligned=True, mode='train'),\n",
    "                                batch_size=2, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(ImageDataset(os.path.join(data_dir, dataset),\n",
    "                                                  transform=transform, unaligned=True, mode='test'),\n",
    "                                     batch_size=20, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756abcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model('cyclegan', device, dataloader, test_dataloader,channels, img_size, num_blocks)\n",
    "model.load_from('weights/')\n",
    "model.eval(batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32004d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                                transforms.ToTensor()\n",
    "                                ])\n",
    "images = transform(noised).unsqueeze(0).cuda()\n",
    "images = next(iter(test_dataloader))['testA'].cuda()\n",
    "predictions = model.generator_AB(images)\n",
    "predictions = (predictions + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "figsize=(10, 5)\n",
    "plt.figure(figsize=figsize)\n",
    "img_grid = make_grid(predictions[:12], nrow=4, padding=10,pad_value=1)\n",
    "plt.imshow(np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0)),interpolation='nearest',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8417c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(10, 6)\n",
    "plt.figure(figsize=figsize)\n",
    "img_grid = make_grid(images[:12], nrow=4, padding=10,pad_value=1)\n",
    "plt.imshow(np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0)),interpolation='nearest',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_drawn = []\n",
    "#transform = transforms.Resize((64,64))\n",
    "def drawing_figure(image):\n",
    "    gray = image \n",
    "    gray = cv2.resize(gray,(800,800))\n",
    "    th, threshed = cv2.threshold(gray, 40, 255,cv2.THRESH_BINARY)\n",
    "    cnts = cv2.findContours(threshed, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)[-2]\n",
    "    sc =[]\n",
    "    for ind,c in enumerate(cnts):\n",
    "        area = cv2.contourArea(c)\n",
    "        #print(area)\n",
    "        if(area<25000): continue\n",
    "        sc.append(c.copy())\n",
    "    draw_img = np.zeros(gray.shape)\n",
    "    for s in sc:\n",
    "        for pt in s:\n",
    "            draw_img[pt[0][1],pt[0][0]] = 255\n",
    "    return draw_img, sc\n",
    "for i in range(len(images)):\n",
    "    noise_dotted_for_drawing = cv2.resize(images[i][0].cpu().numpy(), dsize=(800,800))\n",
    "    t = predictions[i][0].detach().cpu().numpy()*255\n",
    "    t = t.astype('uint8')\n",
    "    contoured_img, hh = drawing_figure(t) \n",
    "    im2 = cv2.drawContours(noise_dotted_for_drawing, hh, -1, (255, 255, 0), 8)\n",
    "    im2 = torch.tensor(im2).unsqueeze(0)\n",
    "    imgs_drawn.append(im2)\n",
    "\n",
    "figsize=(20, 10)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "img_grid1 = make_grid(imgs_drawn, nrow=10, padding=0)\n",
    "ax = plt.imshow(np.transpose(img_grid1.detach().cpu().numpy(), (1, 2, 0)),cmap='gray')\n",
    "plt.axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
